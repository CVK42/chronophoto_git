{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('donnees_nettete.pkl', 'rb') as fichier:\n",
    "    nettete = pickle.load(fichier)\n",
    "with open('donnees_bruit_HF.pkl', 'rb') as fichier:\n",
    "    bruit = pickle.load(fichier)\n",
    "with open('donnees_resolutions.pkl', 'rb') as fichier:\n",
    "    resolution = pickle.load(fichier)\n",
    "with open('donnees_mean_H.pkl', 'rb') as fichier:\n",
    "    mean_H = pickle.load(fichier)\n",
    "with open('donnees_mean_S.pkl', 'rb') as fichier:\n",
    "    mean_S = pickle.load(fichier)\n",
    "with open('donnees_mean_V.pkl', 'rb') as fichier:\n",
    "    mean_V = pickle.load(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [nettete,bruit,resolution,mean_H,mean_S,mean_V]\n",
    "decennies = []\n",
    "L = [604, 658, 561, 649, 773, 587, 615, 632, 564, 563, 586, 751]\n",
    "for i in range(len(L)):\n",
    "    for j in range(L[i]):\n",
    "        decennies.append(1900 + i * 10) \n",
    "        \n",
    "        \n",
    "ensemble_donnees = []\n",
    "for i in range(len(features[0])):\n",
    "    caracteristiques = []\n",
    "    for feature in features:\n",
    "        caracteristique = feature[i]\n",
    "        caracteristiques.append(caracteristique)\n",
    "    decennie = decennies[i]\n",
    "    donnee = (caracteristiques, decennie)\n",
    "    ensemble_donnees.append(donnee)\n",
    "\n",
    "for i in range (len(features)):\n",
    "    for j in range (len(features[i])):\n",
    "        M=max(features[i])\n",
    "        features[i][j]=features[i][j]/M\n",
    "        \n",
    "ensemble_donnees = []\n",
    "for i in range(len(features[0])):\n",
    "    caracteristiques = []\n",
    "    for feature in features:\n",
    "        caracteristique = feature[i]\n",
    "        caracteristiques.append(caracteristique)\n",
    "    decennie = decennies[i]\n",
    "    donnee = (caracteristiques, decennie)\n",
    "    ensemble_donnees.append(donnee)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m train_data]\n\u001b[0;32m      2\u001b[0m train_labels \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m train_data]\n\u001b[0;32m      3\u001b[0m test_features \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m test_data]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_features = [x[0] for x in train_data]\n",
    "train_labels = [x[1] for x in train_data]\n",
    "test_features = [x[0] for x in test_data]\n",
    "test_labels = [x[1] for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6689726137033156,\n",
       " 0.4961028097943859,\n",
       " 0.546346048551013,\n",
       " 0.41722387709219505,\n",
       " 0.5883190608195672,\n",
       " 0.6700308342683322,\n",
       " 0.5238307824616936,\n",
       " 0.41355305849564583,\n",
       " 0.4867483908344834,\n",
       " 0.550378611118569,\n",
       " 0.5409532881111709,\n",
       " 0.6127696721038576,\n",
       " 0.3912502521270945,\n",
       " 0.5165794170127268,\n",
       " 0.6251040615764224,\n",
       " 0.6675191887321548,\n",
       " 0.6097197781285268,\n",
       " 0.7376314389691976,\n",
       " 0.5358754185472666,\n",
       " 0.4500942445354435,\n",
       " 0.601164168569336,\n",
       " 0.47441380365471514,\n",
       " 0.48220183607607325,\n",
       " 0.5858988762140889,\n",
       " 0.7328326216483809,\n",
       " 0.5811958067965792,\n",
       " 0.4677223702050557,\n",
       " 0.7051414284308927,\n",
       " 0.662836406765418,\n",
       " 0.6430458362850897,\n",
       " 0.4757670262081964,\n",
       " 0.4507820025844193,\n",
       " 0.6486757357280832,\n",
       " 0.6195536177395882,\n",
       " 0.4595366944465778,\n",
       " 0.5833683320880583,\n",
       " 0.6382854304911972,\n",
       " 0.5472549729466345,\n",
       " 0.6745033822048525,\n",
       " 0.7718488931234586,\n",
       " 0.6915903788506632,\n",
       " 0.4288806831241553,\n",
       " 0.7957371226187457,\n",
       " 0.6347519790182815,\n",
       " 0.5698429880076352,\n",
       " 0.35275426895289214,\n",
       " 0.5345829469276928,\n",
       " 0.6253862787573125,\n",
       " 0.48297113027532107,\n",
       " 0.5676982469785635,\n",
       " 0.8169125605525622,\n",
       " 0.753742121839186,\n",
       " 0.36096932879811655,\n",
       " 0.7122097268644049,\n",
       " 0.4546687479274095,\n",
       " 0.6784500000497389,\n",
       " 0.5766694216158792,\n",
       " 0.4325633094160305,\n",
       " 0.7073988021840917,\n",
       " 0.7985776692487836,\n",
       " 0.5799376786305327,\n",
       " 0.48061630193217747,\n",
       " 0.6168652458020968,\n",
       " 0.5399863196233236,\n",
       " 0.7623874935065754,\n",
       " 0.5223949421222956,\n",
       " 0.5588456545839217,\n",
       " 0.38051396969636253,\n",
       " 0.800897754427894,\n",
       " 0.5360051707007417,\n",
       " 0.5387843350040185,\n",
       " 0.7288285080759497,\n",
       " 0.5527996291029149,\n",
       " 0.7972928119054825,\n",
       " 0.5527889213441849,\n",
       " 0.6263499915774589,\n",
       " 0.28071321605167227,\n",
       " 0.44713800189737263,\n",
       " 0.46493702749568727,\n",
       " 0.44082055071701887,\n",
       " 0.3806077627689309,\n",
       " 0.5788238725504767,\n",
       " 0.4145403071049531,\n",
       " 0.44454924734609197,\n",
       " 0.5324924914474184,\n",
       " 0.6570994279927239,\n",
       " 0.6129873879429192,\n",
       " 0.5026390205796559,\n",
       " 0.5622405257618717,\n",
       " 0.618883588859289,\n",
       " 0.5091512841393027,\n",
       " 0.4603445179812169,\n",
       " 0.5869157972353688,\n",
       " 0.39492724489907505,\n",
       " 0.40314035829494554,\n",
       " 0.5967034335957306,\n",
       " 0.7836515015239823,\n",
       " 0.6764977105019945,\n",
       " 0.5778242964489138,\n",
       " 0.5892207703820559,\n",
       " 0.5988673136260264,\n",
       " 0.43964892054308663,\n",
       " 0.6042533266261361,\n",
       " 0.6458465457007085,\n",
       " 0.6741809460057717,\n",
       " 0.6063254851039399,\n",
       " 0.6668494136290034,\n",
       " 0.7256895327804774,\n",
       " 0.6296988813539083,\n",
       " 0.3824067172544416,\n",
       " 0.79503256094021,\n",
       " 0.5946297716450221,\n",
       " 0.5033819260599818,\n",
       " 0.763664042097679,\n",
       " 0.740250778625239,\n",
       " 0.5413070413999439,\n",
       " 0.5937850399811002,\n",
       " 0.5357554935652663,\n",
       " 0.3812632747260507,\n",
       " 0.5754261092194312,\n",
       " 0.5368493021780905,\n",
       " 0.5669182539279947,\n",
       " 0.37172194338615233,\n",
       " 0.46446280933992873,\n",
       " 0.5237259277558112,\n",
       " 0.5509892227555683,\n",
       " 0.39366876069711054,\n",
       " 0.5121677544446852,\n",
       " 0.5354908876634458,\n",
       " 0.32414320561132987,\n",
       " 0.5452762837745873,\n",
       " 0.6216976944530453,\n",
       " 0.45642129379968244,\n",
       " 0.5257725767367473,\n",
       " 0.5218746941054713,\n",
       " 0.48459283617018933,\n",
       " 0.48771741486571996,\n",
       " 0.7190757950920551,\n",
       " 0.4752826158191517,\n",
       " 0.4910990773855567,\n",
       " 0.3659725993205653,\n",
       " 0.39327411856207095,\n",
       " 0.5110834253694533,\n",
       " 0.38073424924169247,\n",
       " 0.4988763118477012,\n",
       " 0.5585025157419644,\n",
       " 0.5548961242991144,\n",
       " 0.45406332911970526,\n",
       " 0.21480284281125364,\n",
       " 0.2611507561570768,\n",
       " 0.4410950462142184,\n",
       " 0.5194753959769497,\n",
       " 0.7072582493174666,\n",
       " 0.5683799807262063,\n",
       " 0.512323580753992,\n",
       " 0.47614965756098065,\n",
       " 0.6186003473502841,\n",
       " 0.5591145415737875,\n",
       " 0.6737164136007188,\n",
       " 0.482841176254135,\n",
       " 0.44201509673230016,\n",
       " 0.6355096605852869,\n",
       " 0.44487599653186527,\n",
       " 0.5605643068974933,\n",
       " 0.42030970795869244,\n",
       " 0.5637121805987377,\n",
       " 0.49517311198418906,\n",
       " 0.615376898060605,\n",
       " 0.5005335460693864,\n",
       " 0.31956296135582496,\n",
       " 0.508149379626989,\n",
       " 0.5219720396663093,\n",
       " 0.5030034460512305,\n",
       " 0.664760417100528,\n",
       " 0.6633025170008922,\n",
       " 0.6267791523271519,\n",
       " 0.5497379944165839,\n",
       " 0.4532881934472646,\n",
       " 0.5893930418582974,\n",
       " 0.516834722957858,\n",
       " 0.5861794056154952,\n",
       " 0.616887832740677,\n",
       " 0.4058604620061328,\n",
       " 0.4815051896782558,\n",
       " 0.6336697478726719,\n",
       " 0.4175357196570198,\n",
       " 0.7067418574132581,\n",
       " 0.4566688099043639,\n",
       " 0.5703852660093003,\n",
       " 0.34635504807131656,\n",
       " 0.4594345563034767,\n",
       " 0.78389633418919,\n",
       " 0.4511592580170056,\n",
       " 0.36900514276309476,\n",
       " 0.4616590773497015,\n",
       " 0.37325281687260126,\n",
       " 0.48520267827776953,\n",
       " 0.6229921977450535,\n",
       " 0.6215984677615557,\n",
       " 0.5489058141806238,\n",
       " 0.6610042275377467,\n",
       " 0.6743897716874049,\n",
       " 0.2566342935463324,\n",
       " 0.6538439852290898,\n",
       " 0.6671864537803047,\n",
       " 0.46536559978919306,\n",
       " 0.5091363262932218,\n",
       " 0.6514047524625838,\n",
       " 0.39558203444424006,\n",
       " 0.5003672885744249,\n",
       " 0.6034183025930695,\n",
       " 0.6036264317745162,\n",
       " 0.6152591835417724,\n",
       " 0.5351505050507323,\n",
       " 0.5955530375093423,\n",
       " 0.554514069154341,\n",
       " 0.6211945649137433,\n",
       " 0.3323725680218449,\n",
       " 0.451905680030825,\n",
       " 0.7076293435882807,\n",
       " 0.504750168840567,\n",
       " 0.45572096061926765,\n",
       " 0.6080494721579428,\n",
       " 0.41766857829824916,\n",
       " 0.3994588195617692,\n",
       " 0.5245801098588565,\n",
       " 0.40555009401200054,\n",
       " 0.7121530002496349,\n",
       " 0.6204476322773194,\n",
       " 0.8633219955162287,\n",
       " 0.7110495128037173,\n",
       " 0.47839679925648904,\n",
       " 0.39382830531098584,\n",
       " 0.4287555885145661,\n",
       " 0.5155875936649288,\n",
       " 0.3179223840767374,\n",
       " 0.41942472651762525,\n",
       " 0.5326584081042098,\n",
       " 0.42971330795446294,\n",
       " 0.4508078703969307,\n",
       " 0.5688952348109186,\n",
       " 0.6670100976029973,\n",
       " 0.7006312057636656,\n",
       " 0.572156233695608,\n",
       " 0.8323548576573474,\n",
       " 0.8460006236732416,\n",
       " 0.6506591595030169,\n",
       " 0.4954461778785802,\n",
       " 0.4983802032038774,\n",
       " 0.5648285284240528,\n",
       " 0.5834800781793348,\n",
       " 0.6360783337853149,\n",
       " 0.37403296054063384,\n",
       " 0.6532170953310161,\n",
       " 0.7203683273434209,\n",
       " 0.6415732550508522,\n",
       " 0.7635926300453675,\n",
       " 0.7050094517527828,\n",
       " 0.6684901300844083,\n",
       " 0.5588368190881355,\n",
       " 0.6205710368688615,\n",
       " 0.5999347960173317,\n",
       " 0.4636346071652958,\n",
       " 0.4511837188709094,\n",
       " 0.6498013479477305,\n",
       " 0.6646152033898942,\n",
       " 0.5841352709243182,\n",
       " 0.6637758152957313,\n",
       " 0.6212227115283184,\n",
       " 0.5127395354205476,\n",
       " 0.5675935290862213,\n",
       " 0.571338302411708,\n",
       " 0.41655328629501437,\n",
       " 0.3829230915374428,\n",
       " 0.5346955988151826,\n",
       " 0.7053629051320295,\n",
       " 0.501022491492235,\n",
       " 0.5281832704773712,\n",
       " 0.2268835804647572,\n",
       " 0.5409182420123433,\n",
       " 0.2999825738942218,\n",
       " 0.39079084395136293,\n",
       " 0.7998799159928246,\n",
       " 0.5332065746262897,\n",
       " 0.46017286980824373,\n",
       " 0.39155498424684854,\n",
       " 0.7554412625210742,\n",
       " 0.5425237739616056,\n",
       " 0.6593484679787976,\n",
       " 0.5122863993914656,\n",
       " 0.48622291401168305,\n",
       " 0.420115214320211,\n",
       " 0.4654478362775368,\n",
       " 0.9536680770442815,\n",
       " 0.4984609728954067,\n",
       " 0.6077036318445562,\n",
       " 0.6062457472429965,\n",
       " 0.6808095477056938,\n",
       " 0.48471173720195204,\n",
       " 0.6582265271101959,\n",
       " 0.4397017164455885,\n",
       " 0.7332813667889583,\n",
       " 0.655288491443221,\n",
       " 0.6231418673722615,\n",
       " 0.5243969534844952,\n",
       " 0.5056423037709553,\n",
       " 0.6545805853891395,\n",
       " 0.5608314978465493,\n",
       " 0.32719993955471094,\n",
       " 0.6070096040630028,\n",
       " 0.48679229980614663,\n",
       " 0.7437481245998488,\n",
       " 0.42321591461465424,\n",
       " 0.5576589206552562,\n",
       " 0.5467095324395117,\n",
       " 0.395417311604291,\n",
       " 0.5586730999709614,\n",
       " 0.6951834105558732,\n",
       " 0.4705358680110971,\n",
       " 0.41632919970850923,\n",
       " 0.5240995215300791,\n",
       " 0.702920213554938,\n",
       " 0.6677138005153639,\n",
       " 0.24019333716836397,\n",
       " 0.5598286038367425,\n",
       " 0.4829703456403237,\n",
       " 0.5490743975923427,\n",
       " 0.8063078877216341,\n",
       " 0.5775571173237665,\n",
       " 0.6309228061035848,\n",
       " 0.6069879475355139,\n",
       " 0.7753848519143982,\n",
       " 0.7061053829288432,\n",
       " 0.7099871405671888,\n",
       " 0.6057217615336706,\n",
       " 0.6238880309391226,\n",
       " 0.6496260439774918,\n",
       " 0.7214760539580862,\n",
       " 0.508812761039386,\n",
       " 0.5312896687011487,\n",
       " 0.48925220834904126,\n",
       " 0.8042257263763649,\n",
       " 0.43754937728101057,\n",
       " 0.6584562455215195,\n",
       " 0.6163681987755614,\n",
       " 0.5421723681115745,\n",
       " 0.5638001163665781,\n",
       " 0.6878118827599894,\n",
       " 0.6306709121368526,\n",
       " 0.556981636034497,\n",
       " 0.48648496074402425,\n",
       " 0.582226711571261,\n",
       " 0.7517965987509045,\n",
       " 0.5667462749049526,\n",
       " 0.5913021672354138,\n",
       " 0.4825843466138471,\n",
       " 0.5948954727528859,\n",
       " 0.5124850750561665,\n",
       " 0.5878430376058508,\n",
       " 0.5844777716026907,\n",
       " 0.7056182352940763,\n",
       " 0.43157277529591315,\n",
       " 0.6300891796736178,\n",
       " 0.6329100179820845,\n",
       " 0.48441425695217544,\n",
       " 0.5988943284090761,\n",
       " 0.6489863079060323,\n",
       " 0.3377084556789869,\n",
       " 0.5814936452601921,\n",
       " 0.6296198605818597,\n",
       " 0.5934529891790007,\n",
       " 0.5941479568235738,\n",
       " 0.5915349346260445,\n",
       " 0.7429456827276268,\n",
       " 0.6819262980278437,\n",
       " 0.5486757490230133,\n",
       " 0.46600657047863636,\n",
       " 0.6547884949008124,\n",
       " 0.5807932857578434,\n",
       " 0.4930472236503361,\n",
       " 0.5517912711863581,\n",
       " 0.6819686642850162,\n",
       " 0.571105710822127,\n",
       " 0.40878553747204405,\n",
       " 0.5829917295200777,\n",
       " 0.5729116471069013,\n",
       " 0.6140355924860454,\n",
       " 0.5217321174604169,\n",
       " 0.625474720948715,\n",
       " 0.3694204088776519,\n",
       " 0.6558233212787125,\n",
       " 0.5543840494288993,\n",
       " 0.5517439210179546,\n",
       " 0.5396866669080267,\n",
       " 0.7129939127233414,\n",
       " 0.5571593300277967,\n",
       " 0.605649976713057,\n",
       " 0.5119941633848213,\n",
       " 0.7267662043591752,\n",
       " 0.6766486587611671,\n",
       " 0.5970662518387813,\n",
       " 0.5217921332279176,\n",
       " 0.7313076872470406,\n",
       " 0.6346215086898093,\n",
       " 0.624055722835566,\n",
       " 0.6089948003297057,\n",
       " 0.5624200611572028,\n",
       " 0.5116969674234295,\n",
       " 0.5057513792312159,\n",
       " 0.4816312234552277,\n",
       " 0.6307307032978062,\n",
       " 0.4334955282440956,\n",
       " 0.6297801238263003,\n",
       " 0.46910965737128574,\n",
       " 0.6260328964697491,\n",
       " 0.6977103042343755,\n",
       " 0.5133934040601569,\n",
       " 0.5933263453347641,\n",
       " 0.5727125508593209,\n",
       " 0.6781655419113853,\n",
       " 0.41928583131882186,\n",
       " 0.4685571857966184,\n",
       " 0.6967894577105859,\n",
       " 0.4274518696258696,\n",
       " 0.5577085013122408,\n",
       " 0.4945942121172971,\n",
       " 0.4026741382047584,\n",
       " 0.459875416413803,\n",
       " 0.6085021770561526,\n",
       " 0.5263916183268822,\n",
       " 0.5666609114501819,\n",
       " 0.5643855283786067,\n",
       " 0.7255062683234038,\n",
       " 0.6177838065596012,\n",
       " 0.39399767174991746,\n",
       " 0.6307144993326954,\n",
       " 0.6278630532092835,\n",
       " 0.5845753662758759,\n",
       " 0.4369444972599417,\n",
       " 0.4675920071829879,\n",
       " 0.5778794115761107,\n",
       " 0.6104733292612095,\n",
       " 0.7530950209702061,\n",
       " 0.6447539824456624,\n",
       " 0.5914210886405403,\n",
       " 0.5782642144149546,\n",
       " 0.6493962836414782,\n",
       " 0.6872321175907677,\n",
       " 0.29369067808414784,\n",
       " 0.6444394221081489,\n",
       " 0.4619383704086819,\n",
       " 0.34172391015882375,\n",
       " 0.5458271691255434,\n",
       " 0.6323974189870162,\n",
       " 0.43027139110295215,\n",
       " 0.6569469246430066,\n",
       " 0.5103690754448966,\n",
       " 0.3504798697469834,\n",
       " 0.6616440383341216,\n",
       " 0.579012359840549,\n",
       " 0.5067859048288271,\n",
       " 0.577731531423075,\n",
       " 0.3818588605661596,\n",
       " 0.47763298099119345,\n",
       " 0.5286420403851507,\n",
       " 0.7371865118320148,\n",
       " 0.7419684198306781,\n",
       " 0.4543785400512575,\n",
       " 0.6217712932078028,\n",
       " 0.6437188533590908,\n",
       " 0.4690055358109824,\n",
       " 0.6128359781047592,\n",
       " 0.6292346313351379,\n",
       " 0.6123623408812969,\n",
       " 0.4702992675825739,\n",
       " 0.38554369087386037,\n",
       " 0.6663278163113527,\n",
       " 0.35047970376573373,\n",
       " 0.5277079161676168,\n",
       " 0.4916239140859784,\n",
       " 0.6074327028970161,\n",
       " 0.5578373447681595,\n",
       " 0.4899118739541771,\n",
       " 0.5041589495701958,\n",
       " 0.5303227450571714,\n",
       " 0.7453544187076754,\n",
       " 0.6629254320859751,\n",
       " 0.5503329575740502,\n",
       " 0.38969061025812163,\n",
       " 0.5814439776067117,\n",
       " 0.597830712418193,\n",
       " 0.6256023615504194,\n",
       " 0.628776184626611,\n",
       " 0.5181782425063587,\n",
       " 0.4199472505601128,\n",
       " 0.5497240022129893,\n",
       " 0.5167874918832823,\n",
       " 0.4618134366853981,\n",
       " 0.7312239805186086,\n",
       " 0.5086341317930314,\n",
       " 0.4608442699983393,\n",
       " 0.4973643269245605,\n",
       " 0.6884746369545449,\n",
       " 0.30801213347659184,\n",
       " 0.6718924332114181,\n",
       " 0.5707766674921222,\n",
       " 0.5606777035676693,\n",
       " 0.5621032064042017,\n",
       " 0.5409577182833234,\n",
       " 0.5407015164241875,\n",
       " 0.6180217533715519,\n",
       " 0.3803471788211282,\n",
       " 0.5533802248124352,\n",
       " 0.4549284042387333,\n",
       " 0.5876853542076409,\n",
       " 0.4354963911857011,\n",
       " 0.51597323612232,\n",
       " 0.6213842930420161,\n",
       " 0.514422549007445,\n",
       " 0.7230202055805505,\n",
       " 0.4814091208248351,\n",
       " 0.5750189347797854,\n",
       " 0.5638187538504955,\n",
       " 0.29974371233313357,\n",
       " 0.42364987714130065,\n",
       " 0.6431681310584481,\n",
       " 0.543921860555484,\n",
       " 0.6001597327478284,\n",
       " 0.5076082706995049,\n",
       " 0.4948313846916242,\n",
       " 0.48019376042905065,\n",
       " 0.49850187885711184,\n",
       " 0.38133982816045836,\n",
       " 0.35939396764833814,\n",
       " 0.545015663081645,\n",
       " 0.5619296464947094,\n",
       " 0.6643502684895581,\n",
       " 0.5666880688146173,\n",
       " 0.5687457787008521,\n",
       " 0.5716187807377253,\n",
       " 0.5576292856261075,\n",
       " 0.3319023537593872,\n",
       " 0.6578197303542194,\n",
       " 0.3930706521607134,\n",
       " 0.4099247390392029,\n",
       " 0.32129163465088734,\n",
       " 0.5662613480345696,\n",
       " 0.6994629762630742,\n",
       " 0.7331895918264449,\n",
       " 0.5477564845363655,\n",
       " 0.6952460703247181,\n",
       " 0.7023920594615716,\n",
       " 0.5826517998508164,\n",
       " 0.5765582814490441,\n",
       " 0.5021707149168958,\n",
       " 0.6635198934691604,\n",
       " 0.4030965056118551,\n",
       " 0.7213478772001111,\n",
       " 0.8078771002220357,\n",
       " 0.5280171676647597,\n",
       " 0.6668221592905802,\n",
       " 0.3702786825466223,\n",
       " 0.5434888700196169,\n",
       " 0.66053299882947,\n",
       " 0.4371412429893169,\n",
       " 0.6464039246658929,\n",
       " 0.29989288152982085,\n",
       " 0.5886477267853764,\n",
       " 0.4737426431941851,\n",
       " 0.5675840844643212,\n",
       " 0.4284143733471963,\n",
       " 0.5837333355550125,\n",
       " 0.5680083981377763,\n",
       " 0.40749918313189687,\n",
       " 0.4287861812560501,\n",
       " 0.7018485394991274,\n",
       " 0.6215906847427852,\n",
       " 0.4824474731349964,\n",
       " 0.5348583211327383,\n",
       " 0.7251220240167952,\n",
       " 0.7880798072360026,\n",
       " 0.7086907354399669,\n",
       " 0.5625145285988565,\n",
       " 0.5634628001220725,\n",
       " 0.6664143259394446,\n",
       " 0.42531571509729194,\n",
       " 0.5567094406275841,\n",
       " 0.5312204048389295,\n",
       " 0.5626325780899071,\n",
       " 0.5373495242138066,\n",
       " 0.4801343671074955,\n",
       " 0.5611213261584094,\n",
       " 0.44506535386981105,\n",
       " 0.5716334227251033,\n",
       " 0.6822975677180652,\n",
       " 0.7356372136377435,\n",
       " 0.67282522692484,\n",
       " 0.44634668764235297,\n",
       " 0.4096408359123992,\n",
       " 0.38443317126337434,\n",
       " 0.43370869453234406,\n",
       " 0.41195493707612035,\n",
       " 0.4702287187844318,\n",
       " 0.4466869910081995,\n",
       " 0.480706614133616,\n",
       " 0.6363382821503111,\n",
       " 0.42511937112569453,\n",
       " 0.4351796351357137,\n",
       " 0.4740035246852937,\n",
       " 0.6818615784520625,\n",
       " 0.5384869147310157,\n",
       " 0.5824111634866028,\n",
       " 0.5919863706280579,\n",
       " 0.7087835400631122,\n",
       " 0.6859682415423163,\n",
       " 0.4639374246710791,\n",
       " 0.33047210131810195,\n",
       " 0.5494805397998712,\n",
       " 0.5748850106617575,\n",
       " 0.49623845068042394,\n",
       " 0.518254592968617,\n",
       " 0.5679445478850451,\n",
       " 0.5141497098113598,\n",
       " 0.43272749480885064,\n",
       " 0.6742659556764986,\n",
       " 0.6497168199008059,\n",
       " 0.5066010046114829,\n",
       " 0.5483991374018091,\n",
       " 0.42205610051660114,\n",
       " 0.5003240029097339,\n",
       " 0.49152507335645346,\n",
       " 0.7871205807795185,\n",
       " 0.7870846532877078,\n",
       " 0.68654858626385,\n",
       " 0.5995166257992706,\n",
       " 0.3950473367916916,\n",
       " 0.3129041606243767,\n",
       " 0.7567870280070134,\n",
       " 0.30706894769455617,\n",
       " 0.5371158937262607,\n",
       " 0.7932633800047592,\n",
       " 0.6367683011279863,\n",
       " 0.43786223355959847,\n",
       " 0.5833348411647656,\n",
       " 0.4751316555855384,\n",
       " 0.4706119279398577,\n",
       " 0.4845301434369086,\n",
       " 0.608688804385585,\n",
       " 0.47495151235904726,\n",
       " 0.37319154779038044,\n",
       " 0.5640955440880862,\n",
       " 0.5320317306827563,\n",
       " 0.7026695751822608,\n",
       " 0.5472163729033073,\n",
       " 0.5331157902777404,\n",
       " 0.40936539961266555,\n",
       " 0.4777584797225728,\n",
       " 0.4697333551496611,\n",
       " 0.3887871637692212,\n",
       " 0.5381925330027356,\n",
       " 0.38353955782189547,\n",
       " 0.6061355050888195,\n",
       " 0.5787228013402319,\n",
       " 0.592029643920249,\n",
       " 0.47286642658970524,\n",
       " 0.5444829148118888,\n",
       " 0.5059355133742208,\n",
       " 0.6274415131530447,\n",
       " 0.37916848191640273,\n",
       " 0.5342255667943134,\n",
       " 0.5587016618463115,\n",
       " 0.5029020041706108,\n",
       " 0.615153804662018,\n",
       " 0.557872267807241,\n",
       " 0.5902444548232799,\n",
       " 0.6432073025421249,\n",
       " 0.3836292465139117,\n",
       " 0.5723878526130446,\n",
       " 0.5063332190566473,\n",
       " 0.662488819126687,\n",
       " 0.2555067438614778,\n",
       " 0.5291792331731134,\n",
       " 0.4601935455559975,\n",
       " 0.22014505017071093,\n",
       " 0.6371493578276624,\n",
       " 0.6056161267327203,\n",
       " 0.4169974874739001,\n",
       " 0.38931704601253897,\n",
       " 0.38357541413241836,\n",
       " 0.5696510731510053,\n",
       " 0.6477151141213797,\n",
       " 0.4255457176222572,\n",
       " 0.6584278019839247,\n",
       " 0.3945882371730214,\n",
       " 0.6646407291486759,\n",
       " 0.1992249390882559,\n",
       " 0.4404549931363702,\n",
       " 0.5097514130533237,\n",
       " 0.5887800658899491,\n",
       " 0.43654150280764314,\n",
       " 0.549794392744842,\n",
       " 0.5069025696155286,\n",
       " 0.3622072274387828,\n",
       " 0.5616421442140882,\n",
       " 0.48779243216578266,\n",
       " 0.48775188212448733,\n",
       " 0.4660919564276988,\n",
       " 0.539279577868627,\n",
       " 0.4546319472679245,\n",
       " 0.6311282053315374,\n",
       " 0.46149692740647086,\n",
       " 0.696783765700968,\n",
       " 0.7013906765611468,\n",
       " 0.796112418694397,\n",
       " 0.646300301159301,\n",
       " 0.5194857079563476,\n",
       " 0.4657999290013475,\n",
       " 0.46431235287449163,\n",
       " 0.5175045246236704,\n",
       " 0.5645573508387962,\n",
       " 0.4724342199137815,\n",
       " 0.7518598363659166,\n",
       " 0.6479896639549904,\n",
       " 0.4621525183985561,\n",
       " 0.68611605686004,\n",
       " 0.46095455227090437,\n",
       " 0.5987466446267748,\n",
       " 0.6937973775740861,\n",
       " 0.7645314148923538,\n",
       " 0.3803165510982051,\n",
       " 0.6253654595453306,\n",
       " 0.48290419915108473,\n",
       " 0.5764818805631415,\n",
       " 0.5000218550533503,\n",
       " 0.33440196157937485,\n",
       " 0.41718511745078607,\n",
       " 0.42681728503502714,\n",
       " 0.26229819907441215,\n",
       " 0.5042266112712344,\n",
       " 0.48506976143097913,\n",
       " 0.6825763888533569,\n",
       " 0.4182713794434571,\n",
       " 0.6753647270901975,\n",
       " 0.6178591141585021,\n",
       " 0.6067198838400272,\n",
       " 0.4261360710422389,\n",
       " 0.6919119108874255,\n",
       " 0.5676152783446048,\n",
       " 0.6794214887317519,\n",
       " 0.3858314641377763,\n",
       " 0.6347237944063615,\n",
       " 0.5722399780535395,\n",
       " 0.5739691115885925,\n",
       " 0.5864856644219467,\n",
       " 0.656635392925061,\n",
       " 0.4824573813816296,\n",
       " 0.5270916341805557,\n",
       " 0.4027800476548445,\n",
       " 0.3889188290306959,\n",
       " 0.4982936947289279,\n",
       " 0.4797747855599145,\n",
       " 0.5773180131969174,\n",
       " 0.6532473664095698,\n",
       " 0.6432262176840752,\n",
       " 0.5967457005637009,\n",
       " 0.5290083636652763,\n",
       " 0.5114714805514428,\n",
       " 0.4720358884402741,\n",
       " 0.5886485692144134,\n",
       " 0.514134343137054,\n",
       " 0.5467113507712633,\n",
       " 0.4873410945752123,\n",
       " 0.5739441057430628,\n",
       " 0.4340014500556743,\n",
       " 0.7008327522549503,\n",
       " 0.5516048789002075,\n",
       " 0.4844204200333869,\n",
       " 0.6204966787094166,\n",
       " 0.5316693099117201,\n",
       " 0.45469171888149507,\n",
       " 0.5860581443705353,\n",
       " 0.532016603545908,\n",
       " 0.403389329300146,\n",
       " 0.5955404644535571,\n",
       " 0.5958533452612886,\n",
       " 0.6613812517936937,\n",
       " 0.5900272506683899,\n",
       " 0.35487018288332817,\n",
       " 0.5702118895429062,\n",
       " 0.45795618580579217,\n",
       " 0.5515579305596903,\n",
       " 0.4394761733051962,\n",
       " 0.4782713191871655,\n",
       " 0.5358952952338955,\n",
       " 0.7007745698369361,\n",
       " 0.5039252910711176,\n",
       " 0.6624456268297787,\n",
       " 0.6884974591597179,\n",
       " 0.2886481366486292,\n",
       " 0.40017975018793284,\n",
       " 0.5048003935452349,\n",
       " 0.5396434808117372,\n",
       " 0.7376200255423218,\n",
       " 0.7506112462472743,\n",
       " 0.5720963508559526,\n",
       " 0.6253774669585054,\n",
       " 0.29398014350460977,\n",
       " 0.46626251155664117,\n",
       " 0.453737891223691,\n",
       " 0.6629238418127968,\n",
       " 0.6848991120025096,\n",
       " 0.7355295584686735,\n",
       " 0.444626016761029,\n",
       " 0.5203139332408421,\n",
       " 0.33021414637748386,\n",
       " 0.5967544218608811,\n",
       " 0.6097970404625128,\n",
       " 0.6276622635776965,\n",
       " 0.5984877244839824,\n",
       " 0.5732591482898789,\n",
       " 0.4113835692394665,\n",
       " 0.5146203120651407,\n",
       " 0.6888989233380426,\n",
       " 0.550526528359208,\n",
       " 0.6230331736064718,\n",
       " 0.538859159612568,\n",
       " 0.5013041710289438,\n",
       " 0.5226085697633334,\n",
       " 0.45305741365792257,\n",
       " 0.49560887155614747,\n",
       " 0.6296625846070838,\n",
       " 0.6027355387619004,\n",
       " 0.6540401601437774,\n",
       " 0.5611073287964742,\n",
       " 0.6237004844032866,\n",
       " 0.6046521160291606,\n",
       " 0.6273277730970007,\n",
       " 0.47662965917336586,\n",
       " 0.49337172008079067,\n",
       " 0.38630499937394064,\n",
       " 0.36571763763953546,\n",
       " 0.39486875775442587,\n",
       " 0.6568262771514871,\n",
       " 0.6259077026845672,\n",
       " 0.4825214051222886,\n",
       " 0.5451202665304388,\n",
       " 0.6540048933321533,\n",
       " 0.5783929839669886,\n",
       " 0.5725068735266963,\n",
       " 0.31188984830646826,\n",
       " 0.3398322603622181,\n",
       " 0.638412771372526,\n",
       " 0.5284764223940336,\n",
       " 0.4911070912486872,\n",
       " 0.5374703187487667,\n",
       " 0.6943491029672605,\n",
       " 0.5564944302343344,\n",
       " 0.8289940243676418,\n",
       " 0.5598565877835368,\n",
       " 0.5317093059877135,\n",
       " 0.6449150740835499,\n",
       " 0.6506682323748096,\n",
       " 0.6289518344079654,\n",
       " 0.5587764205096822,\n",
       " 0.6097598911220247,\n",
       " 0.5148984964635701,\n",
       " 0.6721949462373612,\n",
       " 0.4947535499276223,\n",
       " 0.49996281858346503,\n",
       " 0.6897873558807339,\n",
       " 0.4850029991369729,\n",
       " 0.49723582463133875,\n",
       " 0.5903773090028389,\n",
       " 0.38911658516642855,\n",
       " 0.6074632594843836,\n",
       " 0.5409037129595953,\n",
       " 0.6662121742299321,\n",
       " 0.5778813475857866,\n",
       " 0.5047664068464743,\n",
       " 0.4803584106455742,\n",
       " 0.40229533131712414,\n",
       " 0.664993750898594,\n",
       " 0.5314901827779043,\n",
       " 0.6976845094163552,\n",
       " 0.6458944130240013,\n",
       " 0.64103638082651,\n",
       " 0.6675070087305356,\n",
       " 0.714311075330708,\n",
       " 0.5671081161959001,\n",
       " 0.5133435706060735,\n",
       " 0.5238560102115692,\n",
       " 0.41666371997407753,\n",
       " 0.4091854574274413,\n",
       " 0.5935846637533305,\n",
       " 0.5070192952351746,\n",
       " 0.4404524293612513,\n",
       " 0.6373754201165602,\n",
       " 0.603683970422111,\n",
       " 0.6146367450678695,\n",
       " 0.5055998408742113,\n",
       " 0.5270336443850588,\n",
       " 0.3499944145416875,\n",
       " 0.6012412577812055,\n",
       " 0.579441529673803,\n",
       " 0.42936052745043735,\n",
       " 0.7572690673661789,\n",
       " 0.51639307966975,\n",
       " 0.46537819388054974,\n",
       " 0.5587583179915891,\n",
       " 0.5281471769197917,\n",
       " 0.5666444751368667,\n",
       " 0.41639276954124455,\n",
       " 0.39922533692494094,\n",
       " 0.36603040295833533,\n",
       " 0.5915952208796225,\n",
       " 0.5620059492669649,\n",
       " 0.5928654640031049,\n",
       " 0.745146806625558,\n",
       " 0.3827252897124904,\n",
       " 0.5341389043106048,\n",
       " 0.4941898788196065,\n",
       " 0.5674243305234947,\n",
       " 0.4087239447017183,\n",
       " 0.5707117166660512,\n",
       " 0.5941813130656346,\n",
       " 0.3853327380663803,\n",
       " 0.5414400962419988,\n",
       " 0.5553097021730825,\n",
       " 0.5082940114231156,\n",
       " 0.71907833672529,\n",
       " 0.3962594412285688,\n",
       " 0.42792855023188614,\n",
       " 0.5435670690319816,\n",
       " 0.5813780178508173,\n",
       " 0.6124791617787338,\n",
       " 0.48987331017779234,\n",
       " 0.46210613545266715,\n",
       " 0.44059687445008844,\n",
       " 0.6529112105542318,\n",
       " 0.6887930437032698,\n",
       " 0.4653702436338769,\n",
       " 0.4502970845835842,\n",
       " 0.7623011420273881,\n",
       " 0.4810802670340174,\n",
       " 0.5146592010729012,\n",
       " 0.4702233057016096,\n",
       " 0.5734569301512579,\n",
       " 0.39022257793669957,\n",
       " 0.3549975138121285,\n",
       " 0.7409063223568473,\n",
       " 0.6464247062356077,\n",
       " 0.6608897235404478,\n",
       " 0.551775393135722,\n",
       " 0.4429947851352709,\n",
       " 0.5763446643360455,\n",
       " 0.38694915154504517,\n",
       " 0.6604038122247328,\n",
       " 0.4832667884071431,\n",
       " 0.613318712473755,\n",
       " 0.47726724579532154,\n",
       " 0.8047849051035615,\n",
       " 0.5764366038270663,\n",
       " 0.4878502332080722,\n",
       " 0.7158159547085117,\n",
       " 0.6376987463175585,\n",
       " 0.6339973380917205,\n",
       " 0.3557060272582836,\n",
       " 0.6784380051752856,\n",
       " 0.4322421963856679,\n",
       " 0.6803079912242784,\n",
       " 0.5317153006939587,\n",
       " 0.3709783826126541,\n",
       " 0.32436890385969286,\n",
       " 0.6644442173315519,\n",
       " 0.46552417906248084,\n",
       " 0.578907013200491,\n",
       " 0.6307919837868987,\n",
       " 0.44032287211906856,\n",
       " 0.6179203851974727,\n",
       " 0.4011979427489825,\n",
       " 0.5396404167315735,\n",
       " 0.44322605053168296,\n",
       " 0.5293388807091317,\n",
       " 0.683856575179527,\n",
       " 0.5810415307832373,\n",
       " 0.669709455109192,\n",
       " 0.6233789513430864,\n",
       " 0.5239755842808962,\n",
       " 0.5175764063406926,\n",
       " 0.4835586590004289,\n",
       " 0.4176992552134309,\n",
       " 0.43923233560707403,\n",
       " 0.49215828893569363,\n",
       " 0.6333415170850054,\n",
       " 0.33821036323011083,\n",
       " 0.6076885010848107,\n",
       " 0.48082035072668017,\n",
       " 0.4642324840605375,\n",
       " 0.5343223405060381,\n",
       " 0.5705099717236292,\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m rf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Entraîner le modèle sur les données d'entraînement\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m rf\u001b[39m.\u001b[39;49mfit(train_features, train_labels)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Faire des prédictions sur les données de test\u001b[39;00m\n\u001b[0;32m     15\u001b[0m predictions \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mpredict(test_features)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1854\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1857\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1784\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1785\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1786\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(ensemble_donnees, decennies, test_size=0.2, random_state=42)\n",
    "train_features = [x[0] for x in train_data]\n",
    "train_labels = [x[1] for x in train_data]\n",
    "test_features = [x[0] for x in test_data]\n",
    "test_labels = [x[1] for x in test_data]\n",
    "# Créer une instance de RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "rf.fit(train_features, train_labels)\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = rf.predict(test_features)\n",
    "# Évaluer les performances du modèle (calcul de RMSE)\n",
    "rmse = mean_squared_error(test_labels, predictions, squared=False)\n",
    "# Afficher les résultats\n",
    "print(\"RMSE :\", rmse)\n",
    "import pandas as pd\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "results_df = pd.DataFrame({'Vraie valeur': test_labels, 'Valeur prédite': predictions})\n",
    "# Affichage du DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 28.79\n",
      "      Vraie valeur  Valeur prédite\n",
      "0             1940            2000\n",
      "1             1980            1930\n",
      "2             1920            1930\n",
      "3             2010            2010\n",
      "4             2000            1960\n",
      "...            ...             ...\n",
      "1504          1960            1990\n",
      "1505          1960            1930\n",
      "1506          1920            1940\n",
      "1507          2010            2010\n",
      "1508          1940            1970\n",
      "\n",
      "[1509 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(ensemble_donnees, decennies, test_size=0.2, random_state=42)\n",
    "train_features = [x[0] for x in train_data]\n",
    "train_labels = [x[1] for x in train_data]\n",
    "test_features = [x[0] for x in test_data]\n",
    "test_labels = [x[1] for x in test_data]\n",
    "# Créer et entraîner le modèle de régression logistique\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, train_labels)\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = model.predict(test_features)\n",
    "# Calculer l'erreur quadratique moyenne (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, predictions))\n",
    "print(\"RMSE: {:.2f}\".format(rmse))\n",
    "import pandas as pd\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "results_df = pd.DataFrame({'Vraie valeur': test_labels, 'Valeur prédite': predictions})\n",
    "# Affichage du DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 25.62\n",
      "      Vraie valeur  Valeur prédite\n",
      "0             1940     1988.686308\n",
      "1             1980     1931.098172\n",
      "2             1920     1930.862627\n",
      "3             2010     1974.164491\n",
      "4             2000     1968.055355\n",
      "...            ...             ...\n",
      "1504          1960     1972.271982\n",
      "1505          1960     1934.315802\n",
      "1506          1920     1932.488180\n",
      "1507          2010     1985.845609\n",
      "1508          1940     1961.163969\n",
      "\n",
      "[1509 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(ensemble_donnees, decennies, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = [x[0] for x in train_data]\n",
    "train_labels = [x[1] for x in train_data]\n",
    "test_features = [x[0] for x in test_data]\n",
    "test_labels = [x[1] for x in test_data]\n",
    "\n",
    "\n",
    "# Créer et entraîner le modèle de régression logistique\n",
    "model = LinearRegression()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, predictions))\n",
    "print(\"RMSE: {:.2f}\".format(rmse))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "results_df = pd.DataFrame({'Vraie valeur': test_labels, 'Valeur prédite': predictions})\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
